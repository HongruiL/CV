import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

from src.dataset import Nutrition5kDataset, get_transforms
from src.resnet18_fusion_model import ResNet18Fusion, count_parameters
from src.trainer import DepthFusionTrainer


def set_seed(seed):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_single_model(seed, train_csv, val_csv, data_root,
                       batch_size, num_epochs, image_size, device):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"è®­ç»ƒæ¨¡å‹ - éšæœºç§å­ {seed}")
    print(f"ä½¿ç”¨ ResNet18 + Log Transform")
    print('='*60)

    # è®¾ç½®éšæœºç§å­
    set_seed(seed)

    # åˆ›å»ºæ•°æ®é›†ï¼ˆå¯ç”¨logå˜æ¢ï¼‰
    train_dataset = Nutrition5kDataset(
        csv_file=train_csv,
        data_root=data_root,
        split='train',
        transform=get_transforms('train', image_size),
        use_depth=True,
        use_log_transform=True  # å¯ç”¨logå˜æ¢
    )

    val_dataset = Nutrition5kDataset(
        csv_file=val_csv,
        data_root=data_root,
        split='train',
        transform=get_transforms('val', image_size),
        use_depth=True,
        use_log_transform=True  # å¯ç”¨logå˜æ¢
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2,      # âœ“ åŠ é€Ÿæ•°æ®åŠ è½½
        pin_memory=True,    # âœ“ åŠ é€ŸGPUä¼ è¾“
        persistent_workers=True  # âœ“ ä¿æŒworkerè¿›ç¨‹
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        persistent_workers=True
    )

    # åˆ›å»ºResNet18æ¨¡å‹
    model = ResNet18Fusion(dropout_rate=0.5)
    num_params = count_parameters(model)
    print(f"æ¨¡å‹å‚æ•°é‡: {num_params:,}")

    # è®­ç»ƒ
    save_dir = f'checkpoints/resnet18_log_seed{seed}'
    trainer = DepthFusionTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        save_dir=save_dir,
        use_log_transform=True  # å‘Šè¯‰trainerä½¿ç”¨äº†logå˜æ¢
    )

    trainer.train(num_epochs=num_epochs)

    return save_dir


def main():
    DATA_ROOT = Path('Nutrition5K/Nutrition5K')
    TRAIN_CSV = DATA_ROOT / 'nutrition5k_train_clean.csv'
    BATCH_SIZE = 32  # å¢å¤§åˆ°32ï¼Œæ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡
    NUM_EPOCHS = 80   # å‡å°‘åˆ°80ï¼Œå› ä¸ºlræ›´å°äº†
    IMAGE_SIZE = 224
    SEEDS = [42, 123, 456, 789, 2024]  # 5ä¸ªæ¨¡å‹é›†æˆæ•ˆæœæ›´å¥½

    # æ‰“å°è®­ç»ƒé…ç½®
    print("ğŸš€ è®­ç»ƒé…ç½® (ä¿å®ˆç‰ˆ):")
    print(f"  æ¨¡å‹: ResNet18 + Log Transform")
    print(f"  Batch Size: {BATCH_SIZE}")
    print(f"  è®­ç»ƒè½®æ•°: {NUM_EPOCHS}")
    print(f"  æ¨¡å‹æ•°é‡: {len(SEEDS)}")
    print(f"  åˆå§‹å­¦ä¹ ç‡: 0.00002 (è¿›ä¸€æ­¥é™ä½ç¡®ä¿ç¨³å®š)")
    print(f"  ä¼˜åŒ–å™¨: AdamW (æ›´å¥½çš„æ­£åˆ™åŒ–)")
    print(f"  è°ƒåº¦å™¨: ReduceLROnPlateau (patience=5, min_lr=1e-6)")
    print(f"  æŸå¤±å‡½æ•°: SmoothL1Loss (ç¨³å®šç‰ˆ)")
    print(f"  æ·±åº¦é¢„å¤„ç†: ç®€å•å½’ä¸€åŒ– (å›é€€åˆ°ç¨³å®šç‰ˆæœ¬)")
    print(f"  æ•°æ®å¢å¼º: æ¸©å’Œç‰ˆæœ¬ (å‡å°‘è¿‡æ‹Ÿåˆ)")
    print("=" * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"å°†è®­ç»ƒ {len(SEEDS)} ä¸ªResNet18æ¨¡å‹ç”¨äºé›†æˆ")

    # âœ“ ç»Ÿä¸€åˆ’åˆ†æ•°æ®ï¼ˆæ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„è®­ç»ƒ/éªŒè¯é›†ï¼‰
    print("\nåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†...")
    full_df = pd.read_csv(TRAIN_CSV)
    train_df, val_df = train_test_split(
        full_df,
        test_size=0.2,
        random_state=42,  # å›ºå®šç§å­
        shuffle=True
    )

    train_csv_path = DATA_ROOT / 'train_split.csv'
    val_csv_path = DATA_ROOT / 'val_split.csv'
    train_df.to_csv(train_csv_path, index=False)
    val_df.to_csv(val_csv_path, index=False)

    print(f"è®­ç»ƒé›†: {len(train_df)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_df)} æ ·æœ¬")

    # è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ˆç›¸åŒæ•°æ®ï¼Œä¸åŒåˆå§‹åŒ–ï¼‰
    model_dirs = []
    for i, seed in enumerate(SEEDS, 1):
        print(f"\nå¼€å§‹è®­ç»ƒç¬¬ {i}/{len(SEEDS)} ä¸ªæ¨¡å‹")
        try:
            save_dir = train_single_model(
                seed=seed,
                train_csv=train_csv_path,
                val_csv=val_csv_path,
                data_root=DATA_ROOT,
                batch_size=BATCH_SIZE,
                num_epochs=NUM_EPOCHS,
                image_size=IMAGE_SIZE,
                device=device
            )
            model_dirs.append(save_dir)
            print(f"âœ“ æ¨¡å‹ {i} è®­ç»ƒå®Œæˆ")
        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
            break
        except Exception as e:
            print(f"\nâœ— è®­ç»ƒæ¨¡å‹ {i} (seed={seed}) å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # ä¸è¦ç»§ç»­ï¼Œç›´æ¥é€€å‡º
            break

    # ä¿å­˜æ¨¡å‹è·¯å¾„åˆ—è¡¨
    if model_dirs:
        print(f"\n{'='*60}")
        print(f"æˆåŠŸè®­ç»ƒ {len(model_dirs)}/{len(SEEDS)} ä¸ªæ¨¡å‹")
        print(f"{'='*60}")
        print("æ¨¡å‹ä¿å­˜è·¯å¾„:")
        for dir in model_dirs:
            print(f"  - {dir}/best_model.pth")

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open('model_paths.txt', 'w') as f:
            for dir in model_dirs:
                f.write(f"{dir}/best_model.pth\n")

        print("\næ¨¡å‹è·¯å¾„å·²ä¿å­˜åˆ° model_paths.txt")
        print("ä¸‹ä¸€æ­¥ï¼šè¿è¡Œ predict_ensemble_log.py è¿›è¡Œé›†æˆé¢„æµ‹")
    else:
        print("\næ²¡æœ‰æˆåŠŸè®­ç»ƒä»»ä½•æ¨¡å‹ï¼")


def test_single_model():
    """å¿«é€Ÿæµ‹è¯•å•ä¸ªæ¨¡å‹ï¼ˆéªŒè¯ç¨³å®šæ€§æ”¹è¿›ï¼‰"""
    DATA_ROOT = Path('Nutrition5K/Nutrition5K')
    TRAIN_CSV = DATA_ROOT / 'nutrition5k_train_clean.csv'
    BATCH_SIZE = 32
    NUM_EPOCHS = 30  # å¢åŠ åˆ°30ä¸ªepochï¼Œæ›´å¥½è§‚å¯Ÿç¨³å®šæ€§
    IMAGE_SIZE = 224

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"å¿«é€Ÿæµ‹è¯•å•ä¸ªæ¨¡å‹ï¼ˆç¨³å®šæ€§éªŒè¯ï¼‰- è®¾å¤‡: {device}")
    print(f"æµ‹è¯•é…ç½®: {NUM_EPOCHS} epochs, CosineAnnealingWarmRestarts (T_0=15)")

    # ç»Ÿä¸€åˆ’åˆ†æ•°æ®
    full_df = pd.read_csv(TRAIN_CSV)
    train_df, val_df = train_test_split(
        full_df,
        test_size=0.2,
        random_state=42,
        shuffle=True
    )

    train_csv_path = DATA_ROOT / 'train_split.csv'
    val_csv_path = DATA_ROOT / 'val_split.csv'
    train_df.to_csv(train_csv_path, index=False)
    val_df.to_csv(val_csv_path, index=False)

    # è®­ç»ƒå•ä¸ªæ¨¡å‹
    save_dir = train_single_model(
        seed=42,
        train_csv=train_csv_path,
        val_csv=val_csv_path,
        data_root=DATA_ROOT,
        batch_size=BATCH_SIZE,
        num_epochs=NUM_EPOCHS,
        image_size=IMAGE_SIZE,
        device=device
    )

    print(f"\nâœ… å•æ¨¡å‹æµ‹è¯•å®Œæˆï¼æ¨¡å‹ä¿å­˜åˆ°: {save_dir}")
    print("è¯·æ£€æŸ¥è®­ç»ƒæ—¥å¿—ï¼Œç¡®è®¤éªŒè¯æŸå¤±æ˜¯å¦ç¨³å®šä¸‹é™")
    return save_dir


if __name__ == '__main__':
    import sys

    print("ğŸš€ ResNet18 + Log Transform è®­ç»ƒè„šæœ¬")
    print("=" * 50)
    print("ç”¨æ³•:")
    print("  python train_ensemble_log.py        # è®­ç»ƒæ‰€æœ‰5ä¸ªæ¨¡å‹")
    print("  python train_ensemble_log.py test   # å…ˆæµ‹è¯•å•ä¸ªæ¨¡å‹")
    print("=" * 50)

    if len(sys.argv) > 1 and sys.argv[1] == 'test':
        # å¿«é€Ÿæµ‹è¯•å•ä¸ªæ¨¡å‹
        test_single_model()
    else:
        # å®Œæ•´è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        main()
